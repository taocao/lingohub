## English

AI chatbots are already biasing research — we must establish guidelines for their use now

The academic community has looked at how artificial-intelligence tools help researchers to write papers, but not how they distort the literature scientists choose to cite.
By Zhicheng Lin
Twitter Facebook Email
Artificial intelligence (AI) systems are consuming vast amounts of online content yet pointing few users to the articles’ publishers. In early 2025, US-based company OpenAI collected around 250 pages of material for every visitor it directed to a publisher’s website. By mid-2025, that figure had soared to 1,500, according to Matthew Prince, chief executive of US-based Internet-security firm Cloudflare. And the extraction rate of US-based AI start-up company Anthropic climbed even higher over the same period: from 6,000 pages to 60,000. Even tech giant Google, long considered an asset to publishers because of the referral traffic it generated, tripled its ratio from 6 pages to 18 with the launch of its AI Overviews feature. The current information ecosystem is dominated by ‘answer engines’ — AI chatbots that synthesize and deliver information directly, with users trusting the answers now more than ever.

As a researcher in metascience and psychology, I see this transition as the most important change in knowledge discovery in a generation. Although these tools can answer questions faster and often more accurately than search engines can, this efficiency has a price. In addition to the decimation of web traffic to publishers, there is a more insidious cost. Not AI’s ‘hallucinations’ — fabrications that can be corrected — but the biases and vulnerabilities in the real information that these systems present to users.


Will AI speed up literature reviews or derail them entirely?

Consider what happens when researchers ask an AI tool to recommend peer reviewers in their field. One study focused on physics found that AI systems over-represented scholars with names that the scientists classified as belonging to white people and under-represented those with names they classified as Asian (D. Barolo et al. Preprint at arXiv https://doi.org/p46k; 2025). Algorithms can amplify distinct prejudices depending on the field, the context and the wording of the query.

The distortion extends to the literature itself, something I’ve witnessed at first hand. When AI systems recommend research papers, they consistently exacerbate the Matthew effect — the tendency for highly cited work to accumulate even more citations while lesser-known research remains invisible. More than 60% of AI-generated suggestions fall in the top 1% of most-cited articles, more than twice the number seen in human-curated reference lists (A. Algaba et al. Preprint at arXiv https://doi.org/p46m; 2025). AI systems have internalized human citation patterns and have amplified them to an extreme.

Despite the tools’ growing influence, research on AI-assisted information retrieval remains limited. This gap reflects a broader blind spot in how institutions are responding to AI’s growing role in science. Policies have focused on the ethics of AI-assisted article production — establishing principles for responsible use, concerning originality, accountability and transparency, for instance. However, the threat lies not in how we write science, but in how we find it.


Why an overreliance on AI-driven modelling is bad for science

Researchers are scrutinizing AI-generated sentences while implicitly enabling these systems to choose which scholars are cited, which methods seem relevant and which research directions might be promising. They are accepting the outputs even though the underlying information has been distorted. This imbalance is especially concerning because the prospect of autonomous research agents — AI tools designed to conduct literature reviews, and even experiments, at scale — is moving closer to reality.

The solution is not to ban AI tools. That would be both impractical and counterproductive. Instead, guidelines must be developed to harness the power of these emerging systems while actively mitigating their flaws.

First, scientists and funders must spearhead a research agenda that tests how these systems perform in realistic academic workflows. This includes assessing the risks of bias in AI-enabled web search and tackling threats such as prompt injection — hidden, malicious instructions that can manipulate a model’s output.


AI, peer review and the human activity of science

Second, academic institutions must provide training programmes that help researchers to develop skills and strategies for treating AI tools as powerful but fallible assistants. This includes teaching people how to use advanced AI features, including AI-enabled search and deep research tools, and systems such as Google’s NotebookLM — which use a researcher’s own curated documents rather than those on the open web — or how to adapt their prompting skills to AI’s limitations. Rather than asking an AI tool for ‘the most important papers’ on a topic, researchers should learn to prompt the tool for diverse perspectives: ‘What criticisms exist of X theory?’ or ‘Which early-career researchers are working on Y?’

Third, because grant applications increasingly include AI-curated literature summaries, review panels need training to recognize the signs of AI influence: overreliance on a few highly cited papers and the systematic absence of contradictory or methodologically diverse work.

The academic community must recognize that the systems shaping how we access knowledge require the same scrutiny given to the tools shaping how we produce it. The opportunity to shape this outcome is narrowing. Guidelines established now will govern science for years to come.

## 中文


### AI正在扭曲科学的根基，我们却只盯着它写的字

**学术界只关心AI如何“写”论文，却忽略了它如何“喂”给我们文献——一个更隐蔽的危机正在酝酿。**

人工智能（AI）系统正在疯狂吞噬海量的在线内容，却很少将用户引向内容的真正发布者。2025年初，美国公司OpenAI每将一位访客引流至发布商网站，自己就要先“吸走”约250页的材料。而到了2025年中，这个数字飙升至1500页。另一家AI新贵Anthropic的榨取率则更为惊人：同期，其数据从6000页暴增至60000页。就连长期以来被视为出版商“流量贵人”的谷歌，在推出其AI概览功能后，数据榨取率也从6页翻了三倍，达到18页。

我们当下的信息生态，正被一种名为“答案引擎”的怪物所主宰——AI聊天机器人直接合成并提供信息，而用户对这些答案的信任度，也达到了前所未有的高度。

作为一名元科学与心理学研究者，我将这一转变视为我们这一代人在知识发现领域所面临的最深刻变革。尽管这些工具回答问题的速度更快，甚至常常比传统搜索引擎更准确，但这种效率是有代价的。除了出版商网站流量的断崖式下跌外，还有一个更隐蔽的代价。

这个代价，不是AI的“幻觉”——那些尚可被纠正的凭空捏造。而是AI呈现给我们的**真实信息**中所蕴含的偏见与脆弱性。

试想一下，当研究人员让AI工具推荐一位同行评审专家时，会发生什么？一项针对物理学的研究发现，AI系统会过度推荐那些名字听起来像白人的学者，而系统性地忽视那些名字听起来像亚裔的学者。根据领域、上下文和提问方式的不同，算法会放大各种各样的偏见。

这种扭曲，同样延伸到了文献本身，这一点我已亲眼目睹。当AI系统推荐研究论文时，它们会持续加剧“马太效应”——即高被引的论文获得更多引用，而知名度较低的研究则永无出头之日。**超过60%的AI推荐文献，都集中在被引用次数前1%的顶级论文中**，这个比例是人类学者整理的参考文献列表的两倍多。AI不仅学会了人类的引文模式，还将其推向了极致。

尽管这些工具的影响力与日俱增，但关于AI辅助信息检索的研究却寥寥无几。这个缺口，反映出科研机构在应对AI时一个普遍的盲点：**所有政策都聚焦于AI辅助“写作”的伦理问题**——比如原创性、问责制和透明度。然而，真正的威胁，并非我们如何**写**科学，而是我们如何**找**科学。

学者们在字斟句酌地审查AI生成的句子，却在不知不觉中，把挑选哪些学者被引用、判断哪些方法更具价值、指明哪些研究方向更有前景的权力，拱手让给了这些系统。他们正在全盘接受一个已经被扭曲了的信息源。

这种失衡尤其令人担忧，因为能够大规模进行文献综述、甚至设计实验的“自主研究智能体”，正一步步从科幻走向现实。

解决方案不是禁止AI，那既不现实也适得其反。相反，我们必须立即制定指导方针，在驾驭这些新兴系统力量的同时，积极消弭其缺陷。

**第一，科学家和资助机构必须引领一项新的研究议程**，测试这些系统在真实学术工作流中的表现。这包括评估AI搜索中的偏见风险，并应对“提示词注入”（prompt injection）等安全威胁——一种可以操纵模型输出的隐藏恶意指令。

**第二，学术机构必须提供培训**，帮助研究人员将AI视为一个“强大但会犯错的助手”。这包括教授如何使用AI的高级功能，以及如何调整提问技巧以规避其局限性。不要再问AI“关于某某主题最重要的论文是哪些？”，而应该学会引导它提供多元的视角：“关于X理论存在哪些批评？”或者“哪些青年学者正在研究Y方向？”

**第三，由于项目申请书越来越多地包含由AI整理的文献综述，经费评审委员会也需要接受培训**，以识别AI影响的蛛丝马迹：比如，是否过度依赖少数几篇高被引论文，是否系统性地缺失了观点相左或方法多元的研究。

学术界必须清醒地认识到，那些正在塑造我们如何**获取**知识的系统，理应受到和那些塑造我们如何**产出**知识的工具同等级别的审视。

塑造未来的机会窗口正在迅速关闭。我们今天制定的规则，将决定未来数年科学研究的样貌。
