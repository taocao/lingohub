## English

AI agents are coming for your privacy, warns Meredith Whittaker
The Signal Foundation’s president worries they will also blunt competition and undermine cyber-security


Illustration: Dan Williams
Sep 9th 2025

SOON WE WILL all have robot butlers, an army of AI agents anticipating our needs and fulfilling our desires. At least, this is the tech promise of the moment. From booking a restaurant to asking your crush on a date, we’ll be able to put our brain in a jar while a bundle of AI systems does our living for us. Why waste time on wooing when you can leave it to your botservant to turn on the charm? In pursuit of this future, the companies that dominate this market are busy injecting AI agents into the nervous system of the digital world. But as in fairy tales, so in life: relying on magical fixes leads to trouble.

An AI agent is a complex system including AI models, software and cloud infrastructure. For the system to do its thing—summarising your email or spending your money—it needs near-total access to your digital life. This is not the familiar request for permission to see your contacts; it is akin to giving “root” access to your entire device. Your browser history, credit-card details, private messages and location data are all poised to become AI fodder—heaped in an unsecure pile of undifferentiated data “context”.

The push for AI agents comes as the industry is still struggling with profitability. Markets are twitchy, because despite high revenues the huge cost of AI development means pressures are mounting to break even. This helps explain the phantasmagoric promise of agentic AI. It also explains why basic lessons in privacy and digital security are being discarded.

In one sense, the problem is fundamental: there is a powerful tension between privacy and security, on the one hand, and the vision of letting a complex system with broad access to your data do whatever it wants, on the other.

While the full agentic future has not yet arrived, the harms are already clear. Researchers have shown that AI agents can be coaxed into revealing sensitive data they have access to or tricked by hackers into taking harmful actions—from extracting sensitive code to creating havoc in homes by activating smart appliances.

Worse, the threat to communications privacy is real. Security researchers recently exposed Siri transmitting voice transcripts of WhatsApp messages to Apple servers as a part of the rollout of Apple Intelligence, an AI system developed by the firm. This undermines WhatsApp’s end-to-end encryption—adding Apple as another “end” and thus breaking the guarantee that only those sending and receiving communications can access them.

In addition, the way agents are being rolled out is a threat to competition, part of a rush to acquire data by AI giants. Agentic systems are bypassing APIs (short for Application Program Interfaces)—the “front door” for accessing data from third-party apps and services. Instead of paying, these agents could potentially extract competitors’ data in other ways, such as directly accessing whatever is being displayed on their users’ screens. The companies controlling these agents are positioned to aggregate such interface-level data across billions of agentic deployments, generating market insights that those building apps and services understandably don’t want to hand over to rivals.

The threats to privacy, security and competition are heightened by the fact that agents are not being offered as optional apps we can choose to ignore. Operating-system (OS) developers—namely Apple, Google and Microsoft—are integrating them into the core of their platforms, making them all but mandatory.

To put it bluntly, the path currently being taken towards agentic AI leads to an elimination of privacy and security at the application layer. It will not be possible for apps like Signal—the messaging app whose foundation I run—to continue to provide strong privacy guarantees, built on robust and openly validated encryption, if device-makers and OS developers insist on puncturing the metaphoric blood-brain barrier between apps and the OS. Feeding your sensitive Signal messages into an undifferentiated data slurry connected to cloud servers in service of their AI-agent aspirations is a dangerous abdication of responsibility.

Happily, it’s not too late. There is much that can still be done, particularly when it comes to protecting the sanctity of private data. What’s needed is a fundamental shift in how we approach the development and deployment of AI agents. First, privacy must be the default, and control must remain in the hands of application developers exercising agency on behalf of their users. Developers need the ability to designate applications as “sensitive” and mark them as off-limits to agents, at the OS level and otherwise. This cannot be a convoluted workaround buried in settings; it must be a straightforward, well-documented mechanism (similar to Global Privacy Control) that blocks an agent from accessing our data or taking actions within an app.

Second, radical transparency must be the norm. Vague assurances and marketing-speak are no longer acceptable. OS vendors have an obligation to be clear and precise about their architecture and what data their AI agents are accessing, how it is being used and the measures in place to protect it.

These mitigations are the minimum necessary. They should be accompanied by changes in the design of operating systems that improve their ability to shield data from agents and harden their security guarantees, and by serious investment in security research to increase the chances of anticipating, rather than reacting to, vulnerabilities. Without these protections, we risk creating a future in which a few powerful companies decide that the convenience of leaving restaurant-booking or prioritising tasks to AI is more important than cyber-security, healthy competition and the right to private communication. ■

Meredith Whittaker is the president of the Signal Foundation, the non-profit parent organisation of the Signal messaging app.

## 中文

### AI智能管家正在叩门，但代价可能是你的全部隐私

**Signal基金会主席梅雷迪思·惠特克警告：AI智能管家不仅威胁隐私，更将扼杀竞争、掏空网络安全**

很快，我们每个人都将拥有一个机器人管家——一支由人工智能（AI）“智能管家”（AI agent）组成的军队，它们能预判我们的需求，满足我们的欲望。至少，这是科技行业此刻向我们描绘的蓝图。

从预订餐厅到帮你约心上人出门，我们似乎可以把自己的脑子放进罐子里，让一堆AI系统替我们活。既然你的“机器人仆人”就能施展魅力，何必再浪费时间去亲自追求呢？

为了实现这个未来，主导市场的科技巨头们正忙于将AI智能管家注入数字世界的神经网络。然而，童话里的魔法总会带来麻烦，现实生活也是如此。

一个AI智能管家，是一个包含AI模型、软件和云基础设施的复杂系统。要让这个系统为你效劳——无论是总结邮件还是帮你花钱——它都需要近乎完全地访问你的数字生活。这可不是我们平时常见的“请求访问通讯录”，而是近乎于将你整个设备的“最高管理员权限”拱手相让。你的浏览历史、信用卡信息、私密消息和位置数据，都将被扔进一堆未经区分、毫无安全保障的数据“上下文”中，沦为AI的饲料。

这股AI智能管家热潮的背后，是整个行业仍在盈利困境中挣扎的现实。尽管收入高昂，但AI开发的巨额成本意味着收支平衡的压力与日俱增，市场情绪也因此躁动不安。这不仅解释了为何厂商要描绘出智能管家这般梦幻的承诺，也解释了为何隐私与数字安全中最基本的准则，正被它们抛之脑后。

从某种意义上说，这是一个根本性的矛盾：一边是隐私与安全，另一边，则是一个能广泛访问你数据、并为所欲为的复杂系统。这两者之间，存在着巨大的冲突。

尽管那个完全由智能管家驱动的未来尚未到来，但其危害已清晰可见。研究人员已经证明，AI智能管家可以被诱骗泄露它们所能访问的敏感数据，也可能被黑客欺骗去执行有害操作——从窃取敏感代码，到激活智能家电在你的房子里制造混乱。

更糟糕的是，它对通信隐私的威胁是真实存在的。安全研究人员最近就揭露，在苹果公司推广其“苹果智能”（Apple Intelligence）AI系统的过程中，Siri竟会将WhatsApp的语音消息转录本传输至苹果的服务器。这一行为，彻底破坏了WhatsApp的端到端加密——它将苹果公司变成了加密通信的另一个“端点”，从而打破了只有收发双方才能访问信息的安全保证。

此外，AI智能管家的推广方式，也对市场竞争构成了威胁。这已成为AI巨头们数据圈地运动的一部分。这些智能系统正绕过API（应用程序编程接口）——这个访问第三方应用和服务的“正门”。它们不再需要付费合作，而是可能通过其他方式窃取竞争对手的数据，例如，直接访问用户屏幕上显示的任何内容。控制这些智能管家的公司，将能够汇总数十亿次部署所产生的界面层数据，从而获得那些应用开发者绝不愿拱手让给对手的市场洞察。

让这一切雪上加霜的是，智能管家并非一个我们可以选择忽略的可选应用。操作系统（OS）的开发者——即苹果、谷歌和微软——正将它们深度集成到其平台的核心，使其几乎成为强制性的存在。

恕我直言，当前这条通往AI智能管家的道路，终点就是应用层隐私与安全的彻底消亡。如果设备制造商和操作系统开发者，执意要刺穿应用与操作系统之间那道如同“血脑屏障”般的隐喻性壁垒，那么像Signal（我所运营的基金会旗下的即时通讯应用）这样的软件，将无法继续提供基于强大且公开验证的加密技术的隐私保障。为了它们AI智能管家的野心，就将你敏感的Signal消息喂给一个连接着云服务器的、未经区分的数据大杂烩，这是一种危险的、不负责任的放弃。

幸运的是，现在还不算太晚。我们仍有许多事情可做，尤其是在保护私人数据神圣性方面。我们需要的，是对AI智能管家的开发和部署方式，进行一次根本性的转变。

**首先，隐私必须是默认选项，控制权必须掌握在代表用户行使权利的应用程序开发者手中。** 开发者需要有能力在操作系统层面，将应用程序指定为“敏感”，并将其标记为智能管家不可触碰的禁区。这不能是一个深埋在设置里的复杂变通方案，而必须是一个直接、文档清晰的机制（类似于“全球隐私控制”），能够阻止智能管家访问我们的数据或在应用内执行操作。

**其次，必须实现彻底的透明。** 含糊其辞的保证和营销说辞，再也不能被接受。操作系统供应商有义务清晰、准确地说明其架构，他们的AI智能管家正在访问哪些数据，这些数据如何被使用，以及采取了何种措施来保护它。

这些缓解措施，只是最基本的要求。与此同时，操作系统的设计也应做出改变，以提高其保护数据免受智能管家侵扰的能力，并强化其安全保证。此外，还需对安全研究进行严肃投资，以提高我们预测而非被动应对漏洞的可能性。

若没有这些保护，我们恐将创造一个这样的未来：为了让AI代劳预订餐厅或安排任务这点便利，少数几家强大的公司便决定，网络安全、健康竞争和私人通信的权利，都可以被牺牲。

***
*本文作者梅雷迪思·惠特克（Meredith Whittaker）是即时通讯应用Signal的母公司、非营利组织Signal基金会的主席。*
